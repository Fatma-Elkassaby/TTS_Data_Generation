{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc31cf0-0b7a-4a7c-be53-8feab0c9fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                       \n",
    "import torch                    \n",
    "from pydub import AudioSegment, silence\n",
    "from pyannote.audio import Pipeline  \n",
    "import torchaudio\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from pydub.silence import detect_silence\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyannote.audio import Inference\n",
    "import numpy as np\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c417be-5958-4569-a7a9-8894f069d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 5000\n",
    "MAX_LEN = 8000\n",
    "MIN_SILENCE_LEN = 600  \n",
    "SILENCE_THRESH = -35 \n",
    "max_extra_percent=0.15\n",
    "PADDING_MS = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8a31e-1448-48cc-a22d-206c2ffef801",
   "metadata": {},
   "outputs": [],
   "source": [
    "         load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444b33e-cbc9-42d9-8070-3816da698673",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\n",
    "        \"GEMINI_API_KEY not found. Please set it as an environment variable.\"\n",
    "    )\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "print(\" AudioTranscriber initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959c788-1197-456f-a603-07eb83cfb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"medium\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066b421-0661-4592-b292-f170b83e2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "embedding_model = Inference(\n",
    "    \"pyannote/embedding\", \n",
    "    device= device,\n",
    "    use_auth_token=HF_TOKEN)\n",
    "speaker_embeddings_db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91e718-01d2-4b3c-8b04-2d60eabd10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_audio_fixed_length(filepath, chunk_minutes=2, out_dir=\"Final_Output\"):\n",
    "#     podcast_name = Path(filepath).stem \n",
    "#     podcast_dir = os.path.join(out_dir, podcast_name)\n",
    "#     os.makedirs(podcast_dir, exist_ok=True)\n",
    "#     audio = AudioSegment.from_wav(filepath)\n",
    "#     chunk_length = chunk_minutes * 60 * 1000\n",
    "#     chunk_files = []\n",
    "#     start_ms = 0\n",
    "#     chunk_idx = 1\n",
    "#     while start_ms < len(audio):\n",
    "#         end_ms = min(start_ms + chunk_length, len(audio))\n",
    "#         silences = detect_silence(\n",
    "#             audio[start_ms:end_ms],\n",
    "#             min_silence_len=MIN_SILENCE_LEN,\n",
    "#             silence_thresh=SILENCE_THRESH\n",
    "#         )\n",
    "#         if silences:\n",
    "#             last_silence_end = silences[-1][1]\n",
    "#             end_ms = start_ms + last_silence_end\n",
    "#         chunk = audio[start_ms:end_ms]\n",
    "#         chunk_name = f\"chunk{chunk_idx:02d}.wav\"\n",
    "#         chunk_path = os.path.join(podcast_dir, chunk_name)\n",
    "#         chunk.export(chunk_path, format=\"wav\")\n",
    "#         chunk_files.append(chunk_path)\n",
    "#         print(f\" Saved chunk: {chunk_path}\")\n",
    "#         start_ms = end_ms\n",
    "#         chunk_idx += 1\n",
    "#     return chunk_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be283abc-2cfb-4a5b-991c-f751de311543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_segment(segment_audio, min_len=MIN_LEN, max_len=MAX_LEN):\n",
    "    segments = []\n",
    "    audio_len = len(segment_audio)\n",
    "    start = 0\n",
    "    while start < audio_len:\n",
    "        search_end = min(start + max_len, audio_len)\n",
    "        segment = segment_audio[start:search_end]\n",
    "        silences = silence.detect_silence(\n",
    "            segment,\n",
    "            min_silence_len=MIN_SILENCE_LEN,\n",
    "            silence_thresh=SILENCE_THRESH\n",
    "        )\n",
    "        if silences:\n",
    "            cut = start + silences[-1][1]\n",
    "        else:\n",
    "            extra_allowed = int(max_len * 0.2)\n",
    "            search_end_extra = min(start + max_len + extra_allowed, audio_len)\n",
    "            segment_extra = segment_audio[start:search_end_extra]\n",
    "            silences_extra = silence.detect_silence(\n",
    "                segment_extra,\n",
    "                min_silence_len=MIN_SILENCE_LEN,\n",
    "                silence_thresh=SILENCE_THRESH\n",
    "            )\n",
    "            if silences_extra:\n",
    "                cut = start + silences_extra[0][1]\n",
    "            else:\n",
    "                cut = search_end_extra\n",
    "        if cut - start < min_len:\n",
    "            cut = min(start + min_len, audio_len)\n",
    "        segment = segment_audio[start:cut]\n",
    "        segments.append(segment)\n",
    "        start = cut\n",
    "    for i, s in enumerate(segments):\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eac2df-5e71-42b8-a973-714b55c16698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry(stop=stop_after_attempt(3),\n",
    "#        wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "# def transcription(audio_file_path, txt_output_dir):\n",
    "#     \"\"\"\n",
    "#     Transcribes a WAV file, saves transcription to .txt,\n",
    "#     and returns the transcription text.\n",
    "#     \"\"\"\n",
    "#     audio_path = Path(audio_file_path)\n",
    "#     txt_output_dir = Path(txt_output_dir)\n",
    "#     txt_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     txt_file_path = txt_output_dir / f\"{audio_path.stem}.txt\"\n",
    "#     if txt_file_path.exists():\n",
    "#         return txt_file_path.read_text(encoding=\"utf-8\").strip()\n",
    "#     try:\n",
    "#         prompt = \"\"\"\n",
    "#         Please transcribe this Arabic audio accurately.The audio contains Egyptian Arabic dialect speech.\n",
    "\n",
    "#         Instructions:\n",
    "#         1. Transcribe exactly what is spoken in Arabic script.\n",
    "#         2. Do not add any diacritics (tashkeel).\n",
    "#         3. Include natural speech patterns and colloquialisms.\n",
    "#         4. If any English text is spoken, map its characters to Arabic(e.g., 'K' -> 'كي').\n",
    "#         5. Return only the transcribed text, with no additional explanations.\n",
    "#         \"\"\"\n",
    "#         audio_part = {\n",
    "#             \"mime_type\": \"audio/wav\",\n",
    "#             \"data\": audio_path.read_bytes()\n",
    "#         }\n",
    "#         response = model.generate_content([prompt, audio_part])\n",
    "#         text = response.text.strip()\n",
    "#         txt_file_path.write_text(text, encoding=\"utf-8\")\n",
    "#         return text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Transcription failed for {audio_path.name}: {e}\")\n",
    "#         return \"[transcription_failed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd342603-bddf-40a8-9277-3884b6a78704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcription(audio_file_path):\n",
    "    audio_path = Path(audio_file_path)\n",
    "    txt_file_path = audio_path.with_suffix(\".txt\")\n",
    "    if txt_file_path.exists():\n",
    "        return txt_file_path\n",
    "    try:\n",
    "        result = whisper_model.transcribe(str(audio_path), language=None)\n",
    "        text = result['text'].strip()\n",
    "        txt_file_path.write_text(text, encoding=\"utf-8\")\n",
    "        return txt_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription failed for {audio_path.name}: {e}\")\n",
    "        return txt_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cc4a4-b492-40fe-9ae5-57836203652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(segment_audio):\n",
    "    samples = np.array(segment_audio.get_array_of_samples(), dtype=np.float32)\n",
    "\n",
    "    if segment_audio.channels > 1:\n",
    "        samples = samples.reshape(-1, segment_audio.channels).mean(axis=1)\n",
    "\n",
    "    waveform = torch.tensor(samples).unsqueeze(0)\n",
    "\n",
    "    audio_input = {\n",
    "        \"waveform\": waveform,\n",
    "        \"sample_rate\": segment_audio.frame_rate\n",
    "    }\n",
    "\n",
    "    emb = embedding_model(audio_input)\n",
    "    embedding_tensor = torch.tensor(emb.data)\n",
    "    return embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec54a3-65d9-4e13-a5a8-8029f1d0c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_speaker(embedding, speaker_embeddings, threshold=0.65):\n",
    "    emb_mean = embedding.mean(dim=0)\n",
    "    sims = {\n",
    "        spk: torch.nn.functional.cosine_similarity(\n",
    "            emb_mean.unsqueeze(0),\n",
    "            ref.unsqueeze(0)\n",
    "        ).item()\n",
    "        for spk, ref in speaker_embeddings.items()\n",
    "    }\n",
    "    return max(sims, key=sims.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be62bd-d9d9-4775-9dd8-c9256bcbd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = \"speaker_embeddings.pt\"\n",
    "if os.path.exists(embeddings_file):\n",
    "    speaker_embeddings_db = torch.load(embeddings_file)\n",
    "    print(\" Loaded existing speaker embeddings.\")\n",
    "else:\n",
    "    speaker_embeddings_db = {}\n",
    "    print(\"No existing embeddings found, will create new ones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66057291-ac7e-4bf7-aef8-746894177d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = \"Audio_folder\"\n",
    "output_folder = \"Dataset\"\n",
    "Final_Data = pd.DataFrame(\n",
    "    columns=[\"Speaker\", \"Audio_file\", \"Transcription\"]\n",
    ")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "for file_name in os.listdir(audio_folder):\n",
    "    if not file_name.endswith(\".wav\"):\n",
    "        continue\n",
    "    print(f\"\\nProcessing file: {file_name}\") \n",
    "    podcast_name = Path(file_name).stem\n",
    "    file_path = os.path.join(audio_folder, file_name)\n",
    "    podcast_dir = os.path.join(output_folder, podcast_name)\n",
    "    os.makedirs(podcast_dir, exist_ok=True)\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    diarization = pipeline(file_path, num_speakers=2)\n",
    "    print(f\"Diarization done | Number of segments: {len(list(diarization.itertracks(yield_label=True)))}\")\n",
    "    if not speaker_embeddings_db:  \n",
    "    print(\"Creating speaker embeddings for first time...\")\n",
    "    speaker_audio = {\"host\": [], \"guest\": []}\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start_ms = int(turn.start*1000)\n",
    "        end_ms = int(turn.end*1000)\n",
    "        segment_audio = audio[start_ms:end_ms]\n",
    "        sub_segments = split_segment(segment_audio)\n",
    "        if speaker == \"SPEAKER_00\":\n",
    "            speaker_audio[\"host\"].extend(sub_segments)\n",
    "        else:\n",
    "            speaker_audio[\"guest\"].extend(sub_segments)\n",
    "    for spk, segments in speaker_audio.items():\n",
    "        embeddings = []\n",
    "        for seg in segments:\n",
    "            emb = get_embedding(seg)\n",
    "            embeddings.append(emb.mean(dim=0))\n",
    "        speaker_embeddings_db[spk] = torch.stack(embeddings).mean(dim=0)\n",
    "    torch.save(speaker_embeddings_db, embeddings_file)\n",
    "    print(\" Embeddings for first podcast initialized and saved\")\n",
    "    speaker_counter = {\"host\":0, \"guest\":0}\n",
    "    pending_segment = {\"host\": None, \"guest\": None}\n",
    "    speaker_turns = {\"host\": [], \"guest\": []}\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    if speaker == \"SPEAKER_00\":\n",
    "        speaker_turns[\"host\"].append(turn)\n",
    "    else:\n",
    "        speaker_turns[\"guest\"].append(turn)\n",
    "    for spk, turns in speaker_turns.items():\n",
    "    for turn in speaker_turns[spk]:\n",
    "        start_ms = int(turn.start*1000)\n",
    "        end_ms = int(turn.end*1000)\n",
    "        segment_audio = audio[start_ms:end_ms]\n",
    "        emb = get_embedding(segment_audio)\n",
    "        speaker_name = assign_speaker(emb, speaker_embeddings_db)\n",
    "        if pending_segment[speaker_name] is not None:\n",
    "            segment_audio = pending_segment[speaker_name] + segment_audio\n",
    "        pending_segment[speaker_name] = None\n",
    "        sub_segments = split_segment(segment_audio)\n",
    "        sub_len = len(sub_segments)\n",
    "        for i, sub in enumerate(sub_segments):\n",
    "            if len(sub_segments) == 1 and len(sub) < MIN_LEN :\n",
    "                pending_segment[speaker_name] =sub\n",
    "                continue\n",
    "            if i == len(sub_segments) - 1 and len(sub) < MIN_LEN:\n",
    "                pending_segment[speaker_name] =sub\n",
    "                continue\n",
    "            speaker_counter[speaker_name] += 1\n",
    "            audio_file_name = f\"{podcast_name}_{speaker_name}_{speaker_counter[speaker_name]:02d}.wav\"\n",
    "            audio_file_path = os.path.join(podcast_dir, audio_file_name)\n",
    "            sub.export(audio_file_path, format=\"wav\")\n",
    "            print(f\"Segment {speaker_counter[speaker_name]:02d} | Speaker: {speaker_name} | Duration: {len(sub)/1000:.2f}s\")\n",
    "            transcription_file_path = transcription(audio_file_path)\n",
    "            print(f\"Transcription done | File: {audio_file_name} | Transcription path: {transcription_file_path}\")  \n",
    "            Final_Data = pd.concat(\n",
    "                [Final_Data, pd.DataFrame([{\n",
    "                    \"Speaker\": speaker_name,\n",
    "                    \"Audio_file\": audio_file_path,\n",
    "                    \"Transcription\": transcription_file_path\n",
    "                }])],\n",
    "                ignore_index=True\n",
    "            )\n",
    "print(f\" Finished processing file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db64b7-fb29-4048-bcdf-0d888b3cae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(output_folder, \"Dataset.csv\")\n",
    "if os.path.exists(csv_file):\n",
    "    old_data = pd.read_csv(csv_file)\n",
    "    Final_Data = pd.concat([old_data, Final_Data], ignore_index=True)\n",
    "\n",
    "Final_Data.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "print(\"Process Done and data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
